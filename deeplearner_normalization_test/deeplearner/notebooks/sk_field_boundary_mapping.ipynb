{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LUfo70qvVS1s",
    "outputId": "9c352d1f-ab65-4dbc-fb78-ef8a76067179"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/gdrive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KwX84dvdVZtQ"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install rasterio\n",
    "!pip install tensorboardX\n",
    "!pip install boto3\n",
    "!pip install S3Fs\n",
    "!pip install urllib3==1.25.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Q4kB9xwMqE6s"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0xLTdoJVZbck"
   },
   "outputs": [],
   "source": [
    "pd.show_versions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SC1iL8cfVqEr"
   },
   "outputs": [],
   "source": [
    "git_username = \"agroimpacts\"\n",
    "git_token = \"<scrub>\"\n",
    "repo = \"deeplearner\"\n",
    "branch_name = \"normalization_test\"\n",
    "clone_path = \"/content/gdrive/MyDrive/sam/FieldBoundaryDataset/FB_working_folder\"\n",
    "\n",
    "if not os.path.exists(clone_path):\n",
    "    os.makedirs(clone_path)\n",
    "\n",
    "git_path = f'https://{git_token}@github.com/{git_username}/{repo}.git'\n",
    "repo_clone_path = f\"{clone_path}/{repo}\"\n",
    "\n",
    "os.chdir(clone_path)\n",
    "if not os.path.isdir(repo_clone_path):\n",
    "  !git clone -b \"{branch_name}\" \"{git_path}\"\n",
    "else:\n",
    "  if os.listdir(repo_clone_path):\n",
    "    os.chdir(repo_clone_path)\n",
    "    !git branch\n",
    "    !git config core.fileMode false  # needed to ignore permissions changes\n",
    "    !git pull\n",
    "\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y4rXZmhHWGvs"
   },
   "outputs": [],
   "source": [
    "sys.path.insert(0, os.path.join(repo_clone_path, 'deeplearner/'))\n",
    "sys.path.insert(0, repo_clone_path)\n",
    "import deeplearner\n",
    "importlib.reload(deeplearner)\n",
    "from deeplearner.models import *\n",
    "from deeplearner.losses import *\n",
    "from deeplearner.datatorch2 import *\n",
    "from deeplearner.utils import *\n",
    "from deeplearner.compiler import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lrh1bGe9WLUG"
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"source_dir\" : \"/content/gdrive/MyDrive/sam/FieldBoundaryDataset/source\",\n",
    "    \"working_dir\" : \"/content/gdrive/MyDrive/sam/FieldBoundaryDataset/FB_working_folder\",\n",
    "    \n",
    "    # train and validation dataset\n",
    "    \"train_csv_name\" : \"catalog_gh_cg_tz_ng_v1.csv\",\n",
    "    \"train_pickle_name\" : \"train_local_per_tile.pickle\",\n",
    "    \"val_pickle_name\" : \"val_local_per_tile.pickle\",\n",
    "    \"lbl_patchSize\" : 200,\n",
    "    \"one_side_buffer\" : 12,\n",
    "    \"tile_buffer\" : 11,\n",
    "    \"img_path_cols\" : [\"dir_os\"],\n",
    "    \"norm_stats_type\" : \"local_per_tile\",\n",
    "    \"label_path_col\" : \"dir_label\",\n",
    "    \"train_lbl_quality_groups\" : (0, 2, 3, 4),\n",
    "    \"val_lbl_quality_groups\" : (3, 4),\n",
    "    \"transformations\" : ['vflip', 'hflip', 'rotate', 'resize', 'shift_brightness'],\n",
    "    \"rotationDegree\" : (-90, 90),\n",
    "    \"bshift_band_grouping\" : [4],\n",
    "\n",
    "    # train and validation DataLoader\n",
    "    \"train_BatchSize\" : 20,\n",
    "    \"val_BatchSize\" : 2,\n",
    "\n",
    "    # Model\n",
    "    \"input_channels\" : 4,\n",
    "    \"n_classes\" : 3,\n",
    "\n",
    "    # Model compiler\n",
    "    \"gpuDevices\" : [0],\n",
    "    \"params_init_path\" : None,\n",
    "    \"freeze_layer_ls\" : None,\n",
    "    \n",
    "\n",
    "    # Model fitting\n",
    "    \"epochs\" : 5,\n",
    "    \"optimizer\": \"nesterov\",\n",
    "    \"LR\" : 0.01, \n",
    "    \"LR_policy\" : \"PolynomialLR\",\n",
    "    \"criterion\" : \"BalancedTverskyFocalLoss(gamma = 0.9)\",\n",
    "    \"momentum\" : 0.95,\n",
    "    \"resume\" : False,\n",
    "    \"resume_epoch\" : None,\n",
    "    \"bucket\" : \"activemapper\",\n",
    "    \"prefix_out\": \"/content/gdrive/MyDrive/sam/FieldBoundaryDataset/FB_working_folder/ex1_local_per_tile\",\n",
    "\n",
    "    #prediction \n",
    "}\n",
    "\n",
    "pickle_dir = Path(config[\"source_dir\"]) / \"pickles\"\n",
    "if not os.path.exists(pickle_dir):\n",
    "    os.makedirs(pickle_dir)\n",
    "\n",
    "train_pickle_path = pickle_dir / config[\"train_pickle_name\"]\n",
    "val_pickle_path = pickle_dir / config[\"val_pickle_name\"]\n",
    "\n",
    "if not os.path.exists(config[\"working_dir\"]):\n",
    "    os.makedirs(config[\"working_dir\"])\n",
    "\n",
    "log_dir = Path(config[\"working_dir\"]) / \"logs\"\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WJcA5K1i6zEs"
   },
   "source": [
    "# create train and val datasets and pickle them.\n",
    "Only need to be run once if the pickled datasets do not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ji61tRe1W0_F"
   },
   "outputs": [],
   "source": [
    "train_catalog = pd.read_csv(os.path.join(config[\"source_dir\"], \n",
    "                                         config[\"train_csv_name\"]))\n",
    "train_catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YrAgQY_nW24g"
   },
   "outputs": [],
   "source": [
    "# load train dataset\n",
    "train_dataset = planetData(dataPath = config[\"source_dir\"],\n",
    "                           catalog = train_catalog,\n",
    "                           log_dir = log_dir, \n",
    "                           dataSize = config[\"lbl_patchSize\"], \n",
    "                           buffer = config[\"one_side_buffer\"],\n",
    "                           bufferComp = config[\"tile_buffer\"],  \n",
    "                           usage = \"train\", \n",
    "                           imgPathCols = config[\"img_path_cols\"],\n",
    "                           norm_stats_type = config[\"norm_stats_type\"], \n",
    "                           labelPathCol = config[\"label_path_col\"], \n",
    "                           labelGroup = config[\"train_lbl_quality_groups\"], \n",
    "                           deRotate = config[\"rotationDegree\"], \n",
    "                           bShiftSubs = config[\"bshift_band_grouping\"], \n",
    "                           trans=config[\"transformations\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4rSftX02OLjr"
   },
   "outputs": [],
   "source": [
    "pickle_dataset(train_dataset, train_pickle_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nApRb0JHXQ9A"
   },
   "outputs": [],
   "source": [
    "validation_dataset = planetData(dataPath = config[\"source_dir\"],\n",
    "                                log_dir = log_dir,\n",
    "                                catalog = train_catalog, \n",
    "                                dataSize = config[\"lbl_patchSize\"], \n",
    "                                buffer = config[\"one_side_buffer\"],\n",
    "                                bufferComp = config[\"tile_buffer\"],\n",
    "                                usage = \"validate\", \n",
    "                                imgPathCols = config[\"img_path_cols\"],\n",
    "                                labelPathCol = config[\"label_path_col\"], \n",
    "                                labelGroup=config[\"val_lbl_quality_groups\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4efbWHzs-wnQ"
   },
   "outputs": [],
   "source": [
    "pickle_dataset(validation_dataset, val_pickle_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lZjX9goy8CeT"
   },
   "source": [
    "# Walking through the workflow step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TEU3HjsOFBXk"
   },
   "source": [
    "**Step 1.** Setup seeding to make the experiment reproducible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4rI6OmRsFRvq"
   },
   "outputs": [],
   "source": [
    "make_reproducible()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j_4skkAt8Srk"
   },
   "source": [
    "**Step 2.** Load the train and val datasets (i.e. divide the dataset into mini-batches after applying the augmentation, convert to tensor and put them on GPU if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VIL29r0YXK-D"
   },
   "outputs": [],
   "source": [
    "train_dataset = load_dataset(train_pickle_path)\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                              batch_size=config[\"train_BatchSize\"], \n",
    "                              shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-RzHFlbsXVoe"
   },
   "outputs": [],
   "source": [
    "validation_dataset = load_dataset(val_pickle_path)\n",
    "validation_dataloader = DataLoader(validation_dataset, \n",
    "                                 batch_size=config[\"val_BatchSize\"], \n",
    "                                 shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zzUV9cT79JF0"
   },
   "source": [
    "**Step 3.** Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M-AJ8_JDXZwx"
   },
   "outputs": [],
   "source": [
    "model = eval('unet'.lower())(config[\"input_channels\"], config[\"n_classes\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OS_qltnA9bPp"
   },
   "source": [
    "**Step 4.** Compile\n",
    "\n",
    "The model Compiler is responsible for: \n",
    "1) handling the model parallelism on multiple GPUs, \n",
    "2) loading existing model parametrs if needed and\n",
    "3) freeze user-defined layers of model if model-based transfer learning is pursued."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NJp_ZYL3Xhn7"
   },
   "outputs": [],
   "source": [
    "model = ModelCompiler(model, \n",
    "                      buffer = config[\"one_side_buffer\"], \n",
    "                      gpuDevices = config[\"gpuDevices\"], \n",
    "                      params_init = config[\"params_init_path\"],\n",
    "                      freeze_params = config[\"freeze_layer_ls\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mjJXdlPWBbkM"
   },
   "source": [
    "**Step 5.** train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dLzNotwQXnnH"
   },
   "outputs": [],
   "source": [
    "model.fit(train_dataloader, \n",
    "          validation_dataloader, \n",
    "          epochs = config[\"epochs\"], \n",
    "          optimizer_name = config[\"optimizer\"], \n",
    "          lr_init = config[\"LR\"], \n",
    "          lr_policy = config[\"LR_policy\"], \n",
    "          criterion = config[\"criterion\"], \n",
    "          momentum = config[\"momentum\"],\n",
    "          resume = config[\"resume\"], \n",
    "          resume_epoch = config[\"resume_epoch\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6dbXlGrWCOuC"
   },
   "source": [
    "**Step 6.** Save the trained parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T9GklbduXw5j"
   },
   "outputs": [],
   "source": [
    "model.save(bucket=config[\"bucket\"], outPrefix=config[\"prefix_out\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DOxDlWhJBxYS"
   },
   "source": [
    "**Step 7.** Evaluate the trained model against the evaluation dataset and report a number of accuracy metrics in a csv format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9C-XLAYwXvqX"
   },
   "outputs": [],
   "source": [
    "model.evaluate(validate_dataloader, bucket=config[\"bucket\"], prefix_out=config[\"prefix_out\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rJU31PmpCiYr"
   },
   "source": [
    "**Step 8.** Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zJI_yy7hX0nn"
   },
   "outputs": [],
   "source": [
    "def load_pred_data(dir_data, pred_patch_size, pred_buffer, pred_composite_buffer, \n",
    "                   pred_batch, catalog, catalog_row, img_path_cols, average_neighbors=False):\n",
    "    def load_single_tile(catalog_ind = catalog_row):\n",
    "        dataset = planetData(dir_data, catalog, pred_patch_size, pred_buffer, \n",
    "                             pred_composite_buffer, \"predict\", \n",
    "                             catalogIndex=catalog_ind, imgPathCols=img_path_cols)\n",
    "        data_loader = DataLoader(dataset, batch_size=pred_batch, shuffle=False)\n",
    "        meta = dataset.meta\n",
    "        tile = dataset.tile\n",
    "        return data_loader, meta, tile\n",
    "\n",
    "    if average_neighbors == True:\n",
    "        catalog[\"tile_col_row\"] = catalog.apply(lambda x: \"{}_{}\".format(x['tile_col'], x['tile_row']), axis=1)\n",
    "        tile_col = catalog.iloc[catalog_row].tile_col\n",
    "        tile_row = catalog.iloc[catalog_row].tile_row\n",
    "        row_dict = {\n",
    "            \"center\": catalog_row,\n",
    "            \"top\": catalog.query('tile_col=={} & tile_row=={}'.format(tile_col, tile_row - 1)).iloc[0].name \\\n",
    "                if \"{}_{}\".format(tile_col, tile_row - 1) in list(catalog.tile_col_row) else None,\n",
    "            \"left\" : catalog.query('tile_col=={} & tile_row=={}'.format(tile_col - 1, tile_row)).iloc[0].name \\\n",
    "                if \"{}_{}\".format(tile_col - 1, tile_row) in list(catalog.tile_col_row) else None,\n",
    "            \"right\" : catalog.query('tile_col=={} & tile_row=={}'.format(tile_col + 1, tile_row)).iloc[0].name \\\n",
    "                if \"{}_{}\".format(tile_col + 1, tile_row) in list(catalog.tile_col_row) else None,\n",
    "            \"bottom\": catalog.query('tile_col=={} & tile_row=={}'.format(tile_col, tile_row + 1)).iloc[0].name \\\n",
    "                if \"{}_{}\".format(tile_col, tile_row + 1) in list(catalog.tile_col_row) else None,\n",
    "            }\n",
    "        dataset_dict = {k:load_single_tile(catalog_ind = row_dict[k]) if row_dict[k] is not None else None \n",
    "                        for k in row_dict.keys()}\n",
    "        return dataset_dict\n",
    "    # direct crop edge pixels\n",
    "    else:\n",
    "        return load_single_tile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EU-l_ewFYiQY"
   },
   "outputs": [],
   "source": [
    "prefix_out = 'DL/predictions/gh_cg_tz/DFUNet_WithoutAttention_05032022/ghana'\n",
    "catalog = 'catalogs/predict/catalog_ghana_retiled_8.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BB03NBvhYn8p"
   },
   "outputs": [],
   "source": [
    "pred_catalog = pd.read_csv(os.path.join(dir_data, catalog))\n",
    "inds = pred_catalog.query(\"type == 'center'\").index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qBxfyK9OYsZK"
   },
   "outputs": [],
   "source": [
    "for i in inds:\n",
    "    print(\"Predicting on index %s\" % (i))\n",
    "    pred_dataloader = load_pred_data(\n",
    "        dir_data, pred_patch_size, pred_buffer, pred_composite_buffer, \n",
    "        pred_batch, pred_catalog, i, img_path_cols, \n",
    "        average_neighbors = average_neighbors\n",
    "    )\n",
    "    p = model.predict(\n",
    "        pred_dataloader, bucket, prefix_out, \n",
    "        pred_buffer, averageNeighbors=average_neighbors, \n",
    "        shrinkBuffer = shrink_pixels\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wLksaEmtY9jb"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "fig, ax = pyplot.subplots(3,3, figsize=(15,20))\n",
    "for i in range(len(inds)):\n",
    "    col, row = pred_catalog.iloc[inds[i]][['tile_col', 'tile_row']]\n",
    "    image_path = 's3://{}/{}/Score_1/score_c{}_r{}.tif'.\\\n",
    "        format(bucket, prefix_out, col, row)\n",
    "    img = rasterio.open(image_path).read()\n",
    "    show(img.astype('uint8'), ax = ax[math.floor(i/3)][i%3], title=os.path.basename(image_path))\n",
    "    \n",
    "image_path\n",
    "pyplot.savefig(\"score_map_unet.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ukABIH0uZbG7"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "from rasterio.plot import reshape_as_raster, reshape_as_image\n",
    "\n",
    "def findRowCol(filename):\n",
    "    col = re.findall(r\"(?<=_c)\\d\\d\\d(?=_r)\", filename)[0]\n",
    "    row = re.findall(r\"(?<=_r)\\d\\d\\d(?=.tif)\", filename)[0]\n",
    "    return int(row), int(col)\n",
    "\n",
    "def rescale_image(image, bands=(0, 1, 2, 3)):\n",
    "    img = reshape_as_image(image.read())[:,:,bands]\n",
    "    max_vals = [img[:, :, band].max() for band in range(img.shape[-1])]\n",
    "    img = img.astype('float64')\n",
    "    for band in bands:\n",
    "        band_vals = img[:, :, band]\n",
    "        img[:, :, band] = band_vals / max_vals[band]\n",
    "\n",
    "    return img\n",
    "\n",
    "path_to_unet_tiles = 'predicted_tiles_unet'\n",
    "path_to_dfunet_noattn_tiles = 'predicted_tiles_dfunet_noattn'\n",
    "path_to_simpledfunet_tiles = 'predicted_tiles_simpledfunet'\n",
    "\n",
    "predicted_unet = os.listdir(path_to_unet_tiles)\n",
    "predicted_dfunet = os.listdir(path_to_dfunet_noattn_tiles)\n",
    "predicted_simpledfunet = os.listdir(path_to_simpledfunet_tiles)\n",
    "\n",
    "assert(predicted_dfunet == predicted_unet == predicted_simpledfunet)\n",
    "n = len(predicted_unet)\n",
    "assert(len(os.listdir('dir_os')) == len(os.listdir('dir_gs')) == n)\n",
    "\n",
    "fig, ax = pyplot.subplots(n, 4, figsize=(18,40))\n",
    "\n",
    "os_images = []\n",
    "for i in range(n):\n",
    "    unet_path = os.path.join(path_to_unet_tiles, predicted_unet[i])\n",
    "    img = rasterio.open(unet_path).read()\n",
    "    show(img.astype('uint8'), ax = ax[i][0], title=\"UNet_\"+os.path.basename(unet_path)[:-4])\n",
    "    \n",
    "    dfunet_path = os.path.join(path_to_dfunet_noattn_tiles, predicted_unet[i])\n",
    "    img = rasterio.open(dfunet_path).read()\n",
    "    show(img.astype('uint8'), ax = ax[i][1], title=\"DFUNet_NoAttn_\"+os.path.basename(unet_path)[:-4])\n",
    "    \n",
    "    simpledfunet_path = os.path.join(path_to_simpledfunet_tiles, predicted_unet[i])\n",
    "    img = rasterio.open(simpledfunet_path).read()\n",
    "    show(img.astype('uint8'), ax = ax[i][2], title=\"Simple DFUNet_\"+os.path.basename(unet_path)[:-4])\n",
    "    \n",
    "    row, col = findRowCol(unet_path)\n",
    "    # gs_path = os.path.basename(pred_catalog.loc[pred_catalog['tile_col']==col, 'dir_gs'].iloc[0])\n",
    "    # img = rasterio.open(f\"dir_gs/{gs_path}\").read()\n",
    "    # show(img.astype('uint8'), ax = ax[i][3], title=\"GS\")\n",
    "    \n",
    "    os_path = os.path.basename(pred_catalog.loc[pred_catalog['tile_col']==col, 'dir_os'].iloc[0])\n",
    "    img = rasterio.open(f\"dir_os/{os_path}\")\n",
    "    os_images.append(img)\n",
    "    ax_curr = ax[i][3]\n",
    "    img_rescaled = rescale_image(img)\n",
    "    ax_curr.imshow(img_rescaled[:,:,(3,2,1)])\n",
    "\n",
    "pyplot.savefig(\"score_map_compare_simpledfunet.svg\",facecolor='white', format='svg',transparent=False, dpi=300)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
